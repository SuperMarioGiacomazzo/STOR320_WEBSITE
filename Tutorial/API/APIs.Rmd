---
title: "**APIs**"
subtitle: "guest lecture by Marshall Markham"
author: "[STOR 390](https://idc9.github.io/stor390/)"
output:
  html_document:
    theme: cosmo
    toc: yes
    toc_float: yes
---


The acronym API stands for [Application Programming Interface](https://en.wikipedia.org/wiki/Application_programming_interface). An API is simply a way for developers of a software to make that software's functionality available to others programmatically. For instance, the developers of Google Maps, Microsoft Office, and Twitter have created packages in various programming languages to make it so that we can program against their software. 

Obviously, these applications already have interfaces that we know and love. The interfaces we generally interact with in our day to day are GUI's ([Graphical User Interfaces](https://en.wikipedia.org/wiki/Graphical_user_interface)). GUIs tend to be intuitive to use and nice to look at, but they are not good for large workloads. Suppose, for instance, you wanted to code 30,000 addresses into latitude, longitude points. You could easily do this by using Google search for 5 to 10 points. But 30,000 would cause real distress. Fortunately, Google has provided its Maps API which will do this for you in about 5 minutes. More over, the first 2,500 codings are free, and 100,000 is $50! As this example shows, the value of APIs is their ability to remove human interaction from software usage where it is unwanted and/or unneeded.

In this class we will mainly be concerned with API's that grant access to data. In addition to Google's Maps API, one can find API's by Facebook, Yahoo, Tumbler, and many, many more by Google. Private companies are not the only sources of API data access, either. The US Census Bureau has an API which grants us access to demographics info from the various survey's they conduct. Once one starts looking, it becomes apparent that everyone wants to share access to their data.


# **Prerequisites**

You will need to get an [API key for Google maps](https://developers.google.com/maps/documentation/javascript/get-api-key) if you want to run the code in this lecture (the one under `Authentication for the standard API â€” API keys`) . 

```{r setup, warning=FALSE, message=F}
library(jsonlite)

library(tidyverse)

# you should keep your API key a secret
my_key <- 'XXXXXXX' # copy and paste your API key here
```



# **Access**
Alright, well every one wants us to have their data, and we want to have it. Let's go get that data and start making magic! 

... of course there are details to trip us up.

So how does one solve a real world problem using APIs?

## 1. recognize the problem, hope that an API exists, and start Googling
Unlike school, no one is going to tell us to go fix an easy issue with a known API in the work-a-day world. So when we are presented with a problem that requires data we do not posses, we must imagine the data that we want and check whether it has been made available to us. This, arguably, is the most important step in using APIs for analytics.

## 2. read the docs
Once a potential API is discovered, read the documentation to determine whether the data we need is available. The particular fields matter, so we must check what they are. While we are determining what data is available, we should also look for a couple of key pieces of info.

a. What is the request format?
    - Often in the form of a http://..... request, it may also be offered through a package in R, Python, or JavaScript
b. What is the return type
    - Expect json or xml

## 3. sign up if necessary
Many APIs are provided only with authorization, even when access is free.

## 4. request some sample data and review
To start with the actual programming it is best to start with 10 to 100 data points. This will allow us to practice making requests and to get a look at the file format and the data structure. We should expect text file formats to be returned, common formats include json and xml. The structure of the data, may be highly nested and require some work upon receipt to "tiddy" up.

## 5. write formatting functions against the sample data
Use the sample data to write a couple small functions to turn the API's data into a "tiddy" data set. Working on the small data set for the programming tasks will allow you to solve problems in less time, review function output easily during development, and potentially save you from wasting valuable (and possibly limited) requests against the API.

## 6. get the full data set and parse
Once step 5 is complete, make a full request against the API and receive the dataset. Use the functions produced in step 5 to parse the data set. Store the dataset in a useful format (csv, json), in the project files before continuing with analysis. Put your API request code and analysis code in separate files.

# **Google Maps API**

Suppose we are working for a company that has three datasets relating addresses to latitude, longitude coordinates and want to determine the most accurate. What do we do? [Geocoding](https://en.wikipedia.org/wiki/Geocoding).

## STEP 1.
The problem here is that there is no ground truth. How can we know which is THE most accurate, if we do not know where the latitudes and longitudes are supposed to point.

.... if only we had data that told us this thing.

Is there an API which provides reliable or defensible address to lat, lon geocoding? Let's use the Google Maps API.

## STEP 2.

The Google machine will tell you that the docs live here: [https://developers.google.com/maps/documentation/](https://developers.google.com/maps/documentation/). It looks like we have access to geocoding here: [https://developers.google.com/maps/documentation/geocoding/start](https://developers.google.com/maps/documentation/geocoding/start). A sample request looks like this:

>  https://maps.googleapis.com/maps/api/geocode/json?address=1600+Amphitheatre+Parkway,+Mountain+View,+CA&key=YOUR_API_KEY

so we need to use the base string:

> https://maps.googleapis.com/maps/api/geocode/json

with

> address=AN+ADDRESS+OF+INTEREST

and our API key attached

> key=YOUR_API_KEY

it looks like data will come back in JSON format.


## STEP 3.
Let's get those API keys from Google

## STEP 4.
Most of the work will be done here. First we need the set of addresses to investigate. These are found in the file "place_locs.csv"

```{r}
# Load dataset
locs_df = read_csv("place_locs.csv")
head(locs_df)
```

We are going to need to pull addresses from this df, convert them to URLs, request the data, and then transform it for use. We will address these in order.

a. We will build a function to create urls from two arguments, a dataframe row and our Google key. This will cover steps one and two above. Notice the various functionalities used in the building of this routine, they will come in handy the next time this task is necessary.

```{r}
# URL building function
row_to_url <- function(row, key) {
    # set the URL base
    basestring = "https://maps.googleapis.com/maps/api/geocode/json?"
    
    # replace white space with "+" in individual address fields
    street = gsub(" ", "+", row$street)
    city = gsub(" ", "+", row$city) 
    state = gsub(" ", "+", row$state)
    
    # concatenate subfields of address
    address = paste(street, city, state, sep=",+")
    address = paste0("address=", address)

    # create url string from base and addres parts        
    urlstring = paste0(basestring, address)

    # add key to URL and return
    key = paste0("key=", key)
    urlwithkey = paste(urlstring, key, sep="&")
    
    return(urlwithkey)
}
```

Review to ensure proper behavior.

```{r,  cache=F}
row_to_url(locs_df[1, ], my_key)
```

Now lets request some sample data. 
```{r, cache=F}
url_1 = row_to_url(locs_df[1, ], my_key)
url_1
```

We can use the `readLines( )` function to retrieve data from the API by calling `readLines(aURL)`.
```{r}
dat = readLines(url_1)
dat
```

The value `dat` is in string format, we use the `jsonlite` to package to convert.

```{r}
datdf = fromJSON(dat)
```

Looking at the returned data we see that it is, in fact, pretty heavily nested.

```{r}
datdf$results
```

The latitude can be accessed as follows.
```{r}
datdf$results$geometry$location$lat
```

The longitude is accessed similarly.
```{r}
datdf$results$geometry$location$lng
```

And there appears to be a useful location type field here.
```{r}
datdf$results$geometry$location_type
```

## STEP 5.
Now we come to the task of transforming the data for use. For any returned piece of data we will need the lat, lon, and a join key to relate the new info to the inputs. Let's go ahead and collect up the location_type field as well. For a join key we will use the inputs themselves. Ultimately we would like a dataframe that has the join keys and the new info together for use with the initial data. Let's write a couple of functions to do this for us.

The functions we need are:
```{r, eval = FALSE}
# an outer function to control the main process
# recieves the dataframe full of info for request and the google key
# returns a dataframe with the join keys and new data together in a row
map_data <- function(df, key)

# make the request, recieve the json data, parse and tranform to an R dataframe
get_and_parse <- function(urlstring)

# transform a single request data frame into a useful format and append to the dataframe df
# returns the extended dataframe
add_row <- function(df, jsndat)
```

Now let's implement the concepts stubbed out above. We will need some new libraries.
```{r}
get_and_parse <- function(urlstring) {
    dat = readLines(urlstring)
    datdf = fromJSON(dat)
    return(datdf)
}

add_row <- function(df, jsndat, reqdat) {
    # expects df to have columns [street, city, state, lat, lon, loctype]
    # req dat is a row of data used for request [street, city, state]
    resdf = data.frame(
        street = reqdat$street,
        city = reqdat$city,
        state = reqdat$state,
        lat = jsndat$results$geometry$location$lat,
        lon = jsndat$results$geometry$location$lng,
        loctype = jsndat$results$geometry$location_type
    )
    return(rbind(df, resdf))
}

map_data <- function(df, key) {
    # expects df to have columns [street, city, state]
    
    # initialize a dataframe with appropriate columns using the first row of
    # our request data
    reqrow = df[1, c("street", "city", "state")]
    qryurl = row_to_url(reqrow, key)
    jsndf = get_and_parse(qryurl)

    newdf = data.frame(
        street = reqrow$street,
        city = reqrow$city,
        state = reqrow$state,
        lat = jsndf$results$geometry$location$lat,
        lon = jsndf$results$geometry$location$lng,
        loctype = jsndf$results$geometry$location_type
    )

    # populate the dataframe with the rest of the request data
    for (ix in 2:nrow(df)) {
        reqrow = df[ix, c("street", "city", "state")]
        qryurl = row_to_url(reqrow, key)
        jsndf = get_and_parse(qryurl)
        newdf = add_row(newdf, jsndf, reqrow)
    }

    return(newdf)
}
```

Trying our functions on the top 10 rows of the request dataframe yields the desired results.
```{r, cache=F}
map_data(locs_df[1:10, ], my_key)
```

## STEP 6.
At this point it is appropriate to make the full request and pull down the API data. With this code it takes about a minute and a half to pull 100 requests. We are bound by the speed of the network interactions here. Remember to to save off the requested data before continuing with an analysis so that it is not lost.

# **Summary**
As we can see using these web based APIs requires a number of skills. We must be able to find APIs that meet our needs and determine their correct usage. We will need to do some string parsing to for query URLs in order to make the requests. Lastly, we will need to transform the returned data to prep it for analysis.


# **Addtional Resources**
- [R has a long list of packages](https://cran.r-project.org/web/views/WebTechnologies.html) for getting data from the internet.
 
- Jenny Bryan's lecture lecture [on getting data frome the web](http://stat545.com/webdata02_activity.html) is an excellent reference on APIs.

- This [Coursera course](https://www.coursera.org/learn/python-network-data) is also a good reference (though it is in python).

## APIs in R

Here are some other APIs you might find interesting

- Genius (song lyrics)
    - http://genius.engineering/blog/2015/6/5/introducing-the-genius-api-1 
    
- Twitter 
    - http://varianceexplained.org/r/trump-tweets/ 
    - [twitteR](https://cran.r-project.org/web/packages/twitteR/twitteR.pdf)
    - https://dev.twitter.com/rest/public
    
- open  movie data base
    - https://www.omdbapi.com/

- Google Cloud
    - https://cloud.google.com/vision/
    - https://cloud.google.com/speech/
    - https://cloud.google.com/translate/

- Microsoft cognitive services
    - https://www.microsoft.com/cognitive-services/en-us/emotion-api
    - example code in R https://bigdataenthusiast.wordpress.com/2016/10/02/microsoft-cognitive-services-text-analytics-api-in-r/
    - https://github.com/philferriere/mscstexta4r
    - [blog post](http://blog.revolutionanalytics.com/2016/06/programmers-gender.html) uisng microsoft's Face API and [the code](https://github.com/trestletech/eigencoder/blob/master/analysis.Rmd) for that posit
    - another blog post using Face api https://benheubl.github.io/data%20analysis/fr/



## Other References
- https://developers.google.com/maps/documentation

- https://www.r-bloggers.com/using-google-maps-api-and-r/

- https://www.r-bloggers.com/accessing-apis-from-r-and-a-little-r-programming/
- https://www.r-bloggers.com/using-the-httr-package-to-retrieve-data-from-apis-in-r/
- https://cran.r-project.org/web/packages/jsonlite/vignettes/json-apis.html

- https://aws.amazon.com/public-datasets/
- https://ndb.nal.usda.gov/ndb/api/doc

